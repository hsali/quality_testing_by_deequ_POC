{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73861af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f0bcf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_VERSION\"]=\"3.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6d836",
   "metadata": {},
   "source": [
    "# Set up a PySpark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6d572ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a8ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0f261be",
   "metadata": {},
   "outputs": [],
   "source": [
    "jars = glob(\"D:\\work\\sparkapps\\jars\\deequ_jars\\*.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df0f61e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\com.amazon.deequ_deequ-1.2.2-spark-3.0.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\com.chuusai_shapeless_2.12-2.3.2.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\com.github.fommil.netlib_core-1.1.2.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\com.github.rwl_jtransforms-2.4.0.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\junit_junit-4.8.2.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\net.sf.opencsv_opencsv-2.3.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\org.apache.commons_commons-math3-3.2.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\org.scala-lang_scala-reflect-2.12.1.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\org.scalanlp_breeze-macros_2.12-0.13.2.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\org.scalanlp_breeze_2.12-0.13.2.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\org.slf4j_slf4j-api-1.7.5.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\org.spire-math_spire-macros_2.12-0.13.0.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\org.spire-math_spire_2.12-0.13.0.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\org.typelevel_machinist_2.12-0.6.1.jar,D:\\\\work\\\\sparkapps\\\\jars\\\\deequ_jars\\\\org.typelevel_macro-compat_2.12-1.1.1.jar'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(jars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3465ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "#     .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "#     .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "         .config(\"spark.jars\", \",\".join(jars))\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ad2fb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BL8J018:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1cc22aeab80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9fb473b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.sparkContext.parallelize([\n",
    "            Row(a=\"foo\", b=1, c=5),\n",
    "            Row(a=\"bar\", b=2, c=6),\n",
    "            Row(a=\"baz\", b=3, c=None)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b459b7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|  a|  b|   c|\n",
      "+---+---+----+\n",
      "|foo|  1|   5|\n",
      "|bar|  2|   6|\n",
      "|baz|  3|null|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e65a41ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: string (nullable = true)\n",
      " |-- b: long (nullable = true)\n",
      " |-- c: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c0c1fa",
   "metadata": {},
   "source": [
    "# Analyzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b97820e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.analyzers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3b79d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df) \\\n",
    "                    .addAnalyzer(Size()) \\\n",
    "                    .addAnalyzer(Completeness(\"b\")) \\\n",
    "                    .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8e72110",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d287cddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------------+-----+\n",
      "| entity|instance|        name|value|\n",
      "+-------+--------+------------+-----+\n",
      "|Dataset|       *|        Size|  3.0|\n",
      "| Column|       b|Completeness|  1.0|\n",
      "+-------+--------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c7d8c2",
   "metadata": {},
   "source": [
    "# Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "764c704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.profiles import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a70e1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ColumnProfilerRunner(spark) \\\n",
    "    .onData(df) \\\n",
    "    .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3dfd95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardProfiles for column: a: {\n",
      "    \"completeness\": 1.0,\n",
      "    \"approximateNumDistinctValues\": 3,\n",
      "    \"dataType\": \"String\",\n",
      "    \"isDataTypeInferred\": false,\n",
      "    \"typeCounts\": {\n",
      "        \"Boolean\": 0,\n",
      "        \"Fractional\": 0,\n",
      "        \"Integral\": 0,\n",
      "        \"Unknown\": 0,\n",
      "        \"String\": 3\n",
      "    },\n",
      "    \"histogram\": [\n",
      "        [\n",
      "            \"baz\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ],\n",
      "        [\n",
      "            \"foo\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ],\n",
      "        [\n",
      "            \"bar\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ]\n",
      "    ]\n",
      "}\n",
      "NumericProfiles for column: b: {\n",
      "    \"completeness\": 1.0,\n",
      "    \"approximateNumDistinctValues\": 3,\n",
      "    \"dataType\": \"Integral\",\n",
      "    \"isDataTypeInferred\": false,\n",
      "    \"typeCounts\": {},\n",
      "    \"histogram\": [\n",
      "        [\n",
      "            \"1\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ],\n",
      "        [\n",
      "            \"2\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ],\n",
      "        [\n",
      "            \"3\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ]\n",
      "    ],\n",
      "    \"kll\": \"None\",\n",
      "    \"mean\": 2.0,\n",
      "    \"maximum\": 3.0,\n",
      "    \"minimum\": 1.0,\n",
      "    \"sum\": 6.0,\n",
      "    \"stdDev\": 0.816496580927726,\n",
      "    \"approxPercentiles\": []\n",
      "}\n",
      "NumericProfiles for column: c: {\n",
      "    \"completeness\": 0.6666666666666666,\n",
      "    \"approximateNumDistinctValues\": 2,\n",
      "    \"dataType\": \"Integral\",\n",
      "    \"isDataTypeInferred\": false,\n",
      "    \"typeCounts\": {},\n",
      "    \"histogram\": [\n",
      "        [\n",
      "            \"5\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ],\n",
      "        [\n",
      "            \"NullValue\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ],\n",
      "        [\n",
      "            \"6\",\n",
      "            1,\n",
      "            0.3333333333333333\n",
      "        ]\n",
      "    ],\n",
      "    \"kll\": \"None\",\n",
      "    \"mean\": 5.5,\n",
      "    \"maximum\": 6.0,\n",
      "    \"minimum\": 5.0,\n",
      "    \"sum\": 11.0,\n",
      "    \"stdDev\": 0.5,\n",
      "    \"approxPercentiles\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for col, profile in result.profiles.items():\n",
    "    print(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258c341",
   "metadata": {},
   "source": [
    "# Constraint Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "966314c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.suggestions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daabd12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestionResult = ConstraintSuggestionRunner(spark) \\\n",
    "             .onData(df) \\\n",
    "             .addConstraintRule(DEFAULT()) \\\n",
    "             .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af68b21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'constraint_suggestions': [{'constraint_name': 'CompletenessConstraint(Completeness(b,None))',\n",
       "   'column_name': 'b',\n",
       "   'current_value': 'Completeness: 1.0',\n",
       "   'description': \"'b' is not null\",\n",
       "   'suggesting_rule': 'CompleteIfCompleteRule()',\n",
       "   'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint',\n",
       "   'code_for_constraint': '.isComplete(\"b\")'},\n",
       "  {'constraint_name': \"ComplianceConstraint(Compliance('b' has no negative values,b >= 0,None))\",\n",
       "   'column_name': 'b',\n",
       "   'current_value': 'Minimum: 1.0',\n",
       "   'description': \"'b' has no negative values\",\n",
       "   'suggesting_rule': 'NonNegativeNumbersRule()',\n",
       "   'rule_description': 'If we see only non-negative numbers in a column, we suggest a corresponding constraint',\n",
       "   'code_for_constraint': '.isNonNegative(\"b\")'},\n",
       "  {'constraint_name': 'UniquenessConstraint(Uniqueness(List(b),None))',\n",
       "   'column_name': 'b',\n",
       "   'current_value': 'ApproxDistinctness: 1.0',\n",
       "   'description': \"'b' is unique\",\n",
       "   'suggesting_rule': 'UniqueIfApproximatelyUniqueRule()',\n",
       "   'rule_description': 'If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint',\n",
       "   'code_for_constraint': '.isUnique(\"b\")'},\n",
       "  {'constraint_name': 'CompletenessConstraint(Completeness(a,None))',\n",
       "   'column_name': 'a',\n",
       "   'current_value': 'Completeness: 1.0',\n",
       "   'description': \"'a' is not null\",\n",
       "   'suggesting_rule': 'CompleteIfCompleteRule()',\n",
       "   'rule_description': 'If a column is complete in the sample, we suggest a NOT NULL constraint',\n",
       "   'code_for_constraint': '.isComplete(\"a\")'},\n",
       "  {'constraint_name': 'UniquenessConstraint(Uniqueness(List(a),None))',\n",
       "   'column_name': 'a',\n",
       "   'current_value': 'ApproxDistinctness: 1.0',\n",
       "   'description': \"'a' is unique\",\n",
       "   'suggesting_rule': 'UniqueIfApproximatelyUniqueRule()',\n",
       "   'rule_description': 'If the ratio of approximate num distinct values in a column is close to the number of records (within the error of the HLL sketch), we suggest a UNIQUE constraint',\n",
       "   'code_for_constraint': '.isUnique(\"a\")'},\n",
       "  {'constraint_name': \"ComplianceConstraint(Compliance('c' has no negative values,c >= 0,None))\",\n",
       "   'column_name': 'c',\n",
       "   'current_value': 'Minimum: 5.0',\n",
       "   'description': \"'c' has no negative values\",\n",
       "   'suggesting_rule': 'NonNegativeNumbersRule()',\n",
       "   'rule_description': 'If we see only non-negative numbers in a column, we suggest a corresponding constraint',\n",
       "   'code_for_constraint': '.isNonNegative(\"c\")'},\n",
       "  {'constraint_name': 'CompletenessConstraint(Completeness(c,None))',\n",
       "   'column_name': 'c',\n",
       "   'current_value': 'Completeness: 0.6666666666666666',\n",
       "   'description': \"'c' has less than 87% missing values\",\n",
       "   'suggesting_rule': 'RetainCompletenessRule()',\n",
       "   'rule_description': 'If a column is incomplete in the sample, we model its completeness as a binomial variable, estimate a confidence interval and use this to define a lower bound for the completeness',\n",
       "   'code_for_constraint': '.hasCompleteness(\"c\", lambda x: x >= 0.13, \"It should be above 0.13!\")'}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constraint Suggestions in JSON format\n",
    "suggestionResult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d125b0e8",
   "metadata": {},
   "source": [
    "# Constraint Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22d9f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac50e43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87181358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Callback server started!\n"
     ]
    }
   ],
   "source": [
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 3) \\\n",
    "        .hasMin(\"b\", lambda x: x == 0) \\\n",
    "        .isComplete(\"c\")  \\\n",
    "        .isUnique(\"a\")  \\\n",
    "        .isContainedIn(\"a\", [\"foo\", \"bar\", \"baz\"]) \\\n",
    "        .isNonNegative(\"b\")) \\\n",
    "    .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42409e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64dfede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------------------------------------------------------------------------------------------------------+-----------------+-------------------------------------------------------------------+\n",
      "|check       |check_level|check_status|constraint                                                                                                 |constraint_status|constraint_message                                                 |\n",
      "+------------+-----------+------------+-----------------------------------------------------------------------------------------------------------+-----------------+-------------------------------------------------------------------+\n",
      "|Review Check|Warning    |Warning     |SizeConstraint(Size(None))                                                                                 |Success          |                                                                   |\n",
      "|Review Check|Warning    |Warning     |MinimumConstraint(Minimum(b,None))                                                                         |Failure          |Value: 1.0 does not meet the constraint requirement!               |\n",
      "|Review Check|Warning    |Warning     |CompletenessConstraint(Completeness(c,None))                                                               |Failure          |Value: 0.6666666666666666 does not meet the constraint requirement!|\n",
      "|Review Check|Warning    |Warning     |UniquenessConstraint(Uniqueness(List(a),None))                                                             |Success          |                                                                   |\n",
      "|Review Check|Warning    |Warning     |ComplianceConstraint(Compliance(a contained in foo,bar,baz,`a` IS NULL OR `a` IN ('foo','bar','baz'),None))|Success          |                                                                   |\n",
      "|Review Check|Warning    |Warning     |ComplianceConstraint(Compliance(b is non-negative,COALESCE(CAST(b AS DECIMAL(20,10)), 0.0) >= 0,None))     |Success          |                                                                   |\n",
      "+------------+-----------+------------+-----------------------------------------------------------------------------------------------------------+-----------------+-------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkResult_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944307b",
   "metadata": {},
   "source": [
    "# Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f472a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.repository import *\n",
    "from pydeequ.analyzers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27b74ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_file = FileSystemMetricsRepository.helper_metrics_file(spark, 'metrics.json')\n",
    "repository = FileSystemMetricsRepository(spark, metrics_file)\n",
    "key_tags = {'tag': 'pydeequ hello world'}\n",
    "resultKey = ResultKey(spark, ResultKey.current_milli_time(), key_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0b58e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisResult = AnalysisRunner(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addAnalyzer(ApproxCountDistinct('b')) \\\n",
    "    .useRepository(repository) \\\n",
    "    .saveOrAppendResult(resultKey) \\\n",
    "    .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df663868",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_metrep_df = repository.load() \\\n",
    "    .before(ResultKey.current_milli_time()) \\\n",
    "    .forAnalyzers([ApproxCountDistinct('b')]) \\\n",
    "    .getSuccessMetricsAsDataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecab5589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------------------+-----+-------------+-------------------+\n",
      "|entity|instance|               name|value| dataset_date|                tag|\n",
      "+------+--------+-------------------+-----+-------------+-------------------+\n",
      "|Column|       b|ApproxCountDistinct|  3.0|1636830217163|pydeequ hello world|\n",
      "+------+--------+-------------------+-----+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_metrep_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d9491",
   "metadata": {},
   "source": [
    "# wrapping up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094b7645",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext._gateway.shutdown_callback_server()\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
